---
title: "Tutorial"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(ECCCM)
```

### Introduction 

The following tutorial is aimed to explain the use of the `ECCCM` package, most of the functions presented here have additional help available using `?function`. The aim of the package is to conduct inference on SNPs using a reference panel $G_r$. The model assumed is 

$$ y_o = G_o \beta + \epsilon, \quad \epsilon \sim N(0, \sigma^2). $$


The assumption is that only marginal coefficient are reported $\frac{1}{n_o}G_o' y$ . 

#### Data generation 

We will create data according to the model, org or o will refer to the original study. For the purpose of the example, $n_{o} = 5000,n_{r} = 1000, p = 30, \Sigma_{i,j} = 0.8^{|i - j|}, h = 0.05$, where $h$ is the explained variance (that is $\sigma = 1 - h$ for scaled $y$).  There will be two causal SNPs at locations $1, 15$, and the Minor Allele Frequency (MAF) is generated by $p = max(Beta(2, 5) / 3, 0.05)$ (the max is element-wise), to ensure that the SNPs are not empty.  

```{r}
set.seed(998)
#### Parameters 
n.org   <- 5000 
n.ref   <- 1000 
snp.num <- 30 
rho     <- 0.8
h       <- 0.05
beta.vec <- rep(0, snp.num) 
beta.vec[c(1, 15)] <- 1 

cov.mat <- rho^abs(outer(1:snp.num, 1:snp.num, '-'))
X.org   <- MASS::mvrnorm(n.org, rep(0, snp.num), cov.mat) 
X.ref   <- MASS::mvrnorm(n.ref, rep(0, snp.num), cov.mat)
### Transforming to gene 
maf.vec <- pmax(rbeta(snp.num, 2, 5) / 3, 0.05) ## MAF 
#### Create SNPs transforms normal vector into 0,1,2 while keeping MAF
snpMaker <- function(x.vec, maf) {
  g.vec <- x.vec
  g.vec[x.vec <= qnorm(1 - maf)] <- 0
  g.vec[x.vec >  qnorm(1 - maf)]  <- 1
  g.vec[x.vec >  qnorm(1 - (1 / 3) * maf)]  <- 2
  return(g.vec)
}

G.org  <- scale(sweep(X.org, 2, maf.vec, snpMaker))
G.ref  <- scale(sweep(X.ref, 2, maf.vec, snpMaker))

### Creating model 
true.y           <- drop(G.org %*% beta.vec)
unexplain.var    <- (1 - h) / h
y                <- scale(true.y + rnorm(n.org, mean = 0, sd = sqrt(unexplain.var)))


### Reported 
beta.report <- n.org^(-1) * t(G.org) %*% y 

```

#### Estimating variance of coefficients estimate 

The natural estimator of $\beta$ is  

$$\hat{\beta}_{mc} = \frac{n_r}{n_o} (G_r'G_r)^{-1}G_o' y.$$


The naive variance estimator of this estimate is $\frac{\hat{\sigma}^2}{n_o}\hat{\Sigma}_r$. To transform the marginal estimated coefficients to joint estimated coefficients we will use the function `ECCCM::covMatMaker`, the function estimate the covariance matrix using a desired thresholding method and cross validation, for example
After we have obtained our covariance matrix we can transform the marginal associations into multivariate assoications using `ECCCM::marginalToJoint`. 

```{r}
cov.list.r    <- covMatMaker(G.ref, method.threshold = 'soft', threshold.seq = seq(0.01, 0.1, 0.03), 
                             cv.threshold = 5)
### Transform Beta to multivarite
marg.to.joint <- marginalToJoint(marg.beta.hat = beta.report, 
                                 x.r = G.ref, 
                                 n.o = n.org, 
                                 cov.r = cov.list.r$cov, 
                                 inv.r = cov.list.r$omega, 
                                 sigma = 1)
```

To test the coefficients we can use the function `ECCCM::testCoef`, which conduct a two sided normal test. 

```{r}
test.df <- testCoef(est.beta = marg.to.joint$est.beta.hat, 
                    var.beta = marg.to.joint$naive.var.beta.hat,
                    method = 'BH')


which(test.df[ ,3] < 0.05)
```

After correction for multiplicity, we obtain 5 significant SNPs at locations `1, 6, 9, 15 and 30`, implying an FDR of $0.4$. We need to adjust for the variance added from using the reference panel. 

The distribution of $\hat{\beta}_{mc}$ is 


$$    \hat{\beta}_{mc}\mathrel{\dot\sim}  N\left( \beta,   n_{r}^{-1}(\beta' \otimes \Sigma^{-1}) A (\beta \otimes \Sigma^{-1}) + n_{o}^{-1}  (\beta' \Sigma^{-1}  \otimes I_p) A (\Sigma^{-1} \beta \otimes I_p) +  \frac{\sigma^2}{n_o} \Sigma^{-1} \right).
$$


Where, $A =  var \left(vec(G_l G_l')\right)$, since $E(G_l) = 0$, $G_l G_l'$ is the covariance matrix estimator based on a single observation.  $vec$ is the vectorization operator. $vec\left( K\right)$ concatenates the columns of matrix $K \in R^{p \times p}$ to a vector $vec(K) \in R^{p^2}$ and $\otimes$ denotes the Kronecker product. 


After some algebra we can obtain 



$$	 n_{r}^{-1}(\beta' \otimes \Sigma^{-1}) A (\beta \otimes \Sigma^{-1})  =  n_{r}^{-1} \sum_{i = 1}^{p} \sum_{j = 1}^{p} \beta_i \Sigma^{-1} A_{i,j} \Sigma^{-1} \beta_j,$$

and 




$$		n_{o}^{-1}  (\beta' \Sigma^{-1}  \otimes I_p) A (\Sigma^{-1} \beta \otimes I_p) = 	n_{o}^{-1} \sum_{i = 1}^{p} \sum_{j = 1}^{p} (\beta' \Sigma^{-1})_i A_{i,j} (\Sigma^{-1} \beta)_j. $$


The above equations show that using some regularization will help us reduce the computational complexity of the problem (as well as increase power). 

We suggest thresholding the $\hat{\beta}_{mc}$. That is, first test the coefficients using the naive variance estimator at level $q$, use the SNPs for which the test is rejected to estimate the variance, the rest of the coefficients are reduced to 0 (that is, in our example, further test only coefficients corresponding to indices `1, 6, 9, 15 and 30`). 
While assuming that $\beta$ is sparse is reasonable, the same can't be thought of $\Sigma^{-1} \beta$, but, still a few indices of $\Sigma^{-1} \beta$ are enough to capture most of the variance. 


```{r}
threshold.beta.est <- rep(0, snp.num)
threshold.beta.est[which(test.df[ ,3] < 0.05)] <-  marg.to.joint$est.beta.hat[which(test.df[ ,3] < 0.05)]

beta.omega <- cov.list.r$omega %*% threshold.beta.est
weights    <- (beta.omega)^2 / sum((beta.omega)^2)

sort.weight <- sort(weights, decreasing = TRUE, index.return = TRUE)

sum(sort.weight$x[1:8])
```

After thresholding, 8 indices of $\Sigma^{-1} \beta$ are enough to capture `0.93` of the vector norm. 

To estimate the variance we use the function `ECCCM::estimateVarAdd`. The function returns a list with the variance estimates of the two terms above, and the sum of both. 


```{r}
est.var <- estimateVarAdd(beta.mc = threshold.beta.est, 
                          beta.omega = beta.omega,
                          x.r = G.ref, 
                          ind.beta.mc = which(test.df[ ,3] < 0.05), 
                          ind.beta.omega = sort.weight$ix[1:8],
                          omega = cov.list.r$omega,
                          n.o = n.org)
```

And we can test the coefficients with the additional variance 

```{r}
test.df <- testCoef(est.beta = marg.to.joint$est.beta.hat, 
                    var.beta = marg.to.joint$naive.var.beta.hat + diag(est.var$total),
                    method = 'BH')
which(test.df[ ,3] < 0.05)
```

Only SNPs `1 and 15` pass the threshold now. 

#### Summary 

The document demonstrated the usage of various functions of the `ECCCM` package. To save time one can use the `ECCCM::analyzeRef` function that conducts the whole process.
It supports three ways to estimate $\sigma$: 'conservative' ($\hat{\sigma}^2 = 1$), 'estimate' ($\hat{\sigma}^2 = 1 - var(G_r' * \hat{\beta}_{mc})$) and 'semi.conservative' ($\hat{\sigma}^2 = 1 - var(G_r' * \tilde{\beta}_{mc})$, where $\tilde{\beta}_{mc}$ is the thresholded estimated coefficients). 


The function returns details regarding the testing such as the estimated $\sigma$, the number of coefficients and weight captured by them for $\Sigma^{-1} \beta$, as well as the additional variance added and the corrected p-values. 
The function currently only support scaled $X$ and $y$. 


```{r}
head(analyzeRef(marg.beta.hat = beta.report, 
                x.r           = G.ref, 
                n.o           = n.org,
                sigma.method  = 'conservative', 
                qu            = 0.05, 
                explained.omega.beta = 0.95)$test.correct)



head(analyzeRef(marg.beta.hat = beta.report, 
                x.r           = G.ref, 
                n.o           = n.org,
                sigma.method  = 'estimate', 
                qu            = 0.05, 
                explained.omega.beta = 0.95)$test.correct)


head(analyzeRef(marg.beta.hat = beta.report, 
                x.r           = G.ref, 
                n.o           = n.org,
                sigma.method  = 'semi.conservative', 
                qu            = 0.05, 
                explained.omega.beta = 0.95)$test.correct)
```


## Using the normal approximation 


A different approach is to rely on the normal approximation. While the normal approximation is not always suitable and depends on a sufficiently large sample size, it carries benefits of being fast, and not requiring individual level data of the reference panel, solely the LD matrix. 

Using the normal approximation the distribution of $\hat{\beta}_{mc}$, 

$$ 
        \hat{\beta}_{mc} \mathrel{\dot\sim} N\left( \beta,   n_{r}^{-1}(  \beta' \Sigma \beta \Sigma^{-1}) + n_{o}^{-1}  ( \beta' \Sigma^{-1} \beta \Sigma ) + \frac{n_r + n_o}{n_r n_o} \beta \beta' + \frac{\sigma^2}{n_o} \Sigma^{-1} \right).
$$


The function to use is `analyzeRefGauss`, it has less parameters since it does not conducts and regularization, in the example used above. 


```{r}
head(analyzeRefGauss(marg.beta.hat = beta.report, 
                ld.mat        = cov(G.ref), 
                n.o           = n.org,
                n.r           = nrow(G.ref),
                sigma.method  = 'conservative', 
                qu            = 0.05)$test.correct)
```


leading to similar results.
